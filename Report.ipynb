{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning Nanodegree Project 2 - Continuous Control\n",
    "\n",
    "KyoungSu Lee\n",
    "July 12, 2020\n",
    "\n",
    "In this report, I have trained an agent that has continuous action spaces with Deep Deterministic Policy Gradient(DDPG) algorithms. The Reacher of Unity Environment, which is a double jointed arm that moves to target location, is used. I tested with 20 agents to collect experiences in parallel. The agent receives a reward of +0.1 for each step and the project goal was to archieve average scores +30 over 100 consecutive episodes over all agents. (Option 2)\n",
    "\n",
    "Unity Reacher Agent Environment:\n",
    "1. Number or agents: 20\n",
    "1. Size of action for each agent: 4\n",
    "  1. Value between [-1.1]\n",
    "1. Size of states for each agent: 33\n",
    "\n",
    "I tested with multple hidden layers (400,300), (256,128), (128,64), (64,32). I searched other papers and got these hidden layer sets. \n",
    "\n",
    "## Deep Deterministic Policy Gradient (DDPG) Algorithm\n",
    "I trained the actor and the critic network with DDPG algorighm. Actor networks creates policy network that takes state values as input parameter and returns the actions. Critics calculates Q values taking current states and actions that predicted from actor policy as inputs. Target networks simply replica of Actor, Critics were used to reduce the variance of the networks. I used Reply buffer to save experiences from all agents. \n",
    "\n",
    "**Critic loss** - Mean Squared Error of y - Q(s, a) where y is the expected return as seen by the Target network, and Q(s, a) is action value predicted by the Critic network. y is a moving target that the critic model tries to achieve; we make this target stable by updating the Target model slowly.\n",
    "\n",
    "**Actor loss** - This is computed using the mean of the value given by the Critic network for the actions taken by the Actor network. We seek to maximize this quantity.\n",
    "\n",
    "\n",
    "<img src=\"DDPG_Algorithm.png\" width=\"600\"/>\n",
    "\n",
    "\n",
    "## Methods\n",
    "I found many documents without Batch Normalization, the training results were the worst. So I used Batch Normalization for the Actor and the Critic.\n",
    "I  tested with below hidden layers\n",
    "  1. Hidden Layers: 400, 300\n",
    "  1. Hidden Layers: 256, 128\n",
    "  1. Hidden Layers: 128, 64\n",
    "  1. Hidden Layers: 64, 32\n",
    "  \n",
    "\n",
    "## Results\n",
    "\n",
    "The result of agent trainings for DDPG algorithms are shown below. The agents solved the environment in 80 ~ 140 episodes. 80 episodes is enough for finding the goal, +30 scores in 400x300 Hidden Layers. And the Hidden Layers 400x300 was the best performance\n",
    "\n",
    "| hidden layers  | mean_score | first_episode_achieved |\n",
    "|---|---|---|\n",
    "| 400, 300 | 37.7503 | 85 |\n",
    "| 256, 128 | 37.2035 | 111 |\n",
    "| 128, 64 |  35.4912 | 126 |\n",
    "| 64, 32 |   32.6857 | 140 |\n",
    "\n",
    "<img src=\"L-400x300.png\" width=\"500\"/><img src=\"L-256x128.png\"  width=\"500\"/>\n",
    "<img src=\"L-128x64.png\"  width=\"500\"/><img src=\"L-64x32.png\"  width=\"500\"/>\n",
    "\n",
    "## References\n",
    "1. CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING, Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess,Tom Erez, Yuval Tassa, David Silver & Daan WierstraDueling Network Architectures for Deep Reinforcement Learning, Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas https://arxiv.org/pdf/1509.02971.pdf\n",
    "2. Using Deep Reinforcement Learning for the Continuous Control of Robotic Arms, Winfried LÃ¶tzsch, https://arxiv.org/pdf/1810.06746.pdf\n",
    "3. Keras Examples https://keras.io/examples/rl/ddpg_pendulum/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
